import nltk
nltk.download('punkt_tab')
from nltk.tokenize import word_tokenize


text="hello world welcome to my youtube channel"
tokens=word_tokenize(text)
print(tokens)
from nltk.tokenize import word_tokenize


text="hello world welcome to my youtube channel"
tokens=word_tokenize(text)
print(tokens)
char_tokens=list(text)
print(char_tokens[:50])

#subword leve toke 
from tokenizers import ByteLevelBPETokenizer

bpe_tokenizer=ByteLevelBPETokenizer()
bpe_tokenizer.train_from_iterator(text,vocab_size=100,min_frequency=1)
bpe_tokens=bpe_tokenizer.encode(text).tokens
print(bpe_tokens)
